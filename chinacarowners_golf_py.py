# -*- coding: utf-8 -*-
"""ChinaCarOwners_Golf_py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5Jt6vAuwD5r0LugRW03llU2yt-wugWq
"""

import pandas as pd
import numpy as np
import re
from googletrans import Translator
from google.colab import files

# Step 1: Load the CSV file from the specified path, handle mixed types
file_path = '/content/760k-Car-Owners-Nationwide-China-csv-2020.csv'
df = pd.read_csv(file_path, low_memory=False)

# Step 2: Use Google Translate API to translate column headers
translator = Translator()

# Translate column headers from Chinese to English
original_headers = df.columns
translated_headers = [translator.translate(header, src='zh-cn', dest='en').text for header in original_headers]

# Format headers: Replace spaces with underscores and convert to lowercase
formatted_headers = [header.replace(' ', '_').lower() for header in translated_headers]

# Apply the formatted headers to the dataframe
df.columns = formatted_headers

# Step 3: Define a function to clean data
def clean_data(df):
    # Drop duplicates
    df.drop_duplicates(inplace=True)

    # Replace empty or NaN values with placeholder
    df.replace('', np.nan, inplace=True)
    df.dropna(how='all', inplace=True)  # Drop rows where all columns are NaN

    # Further cleaning can be added based on specific data issues like format errors
    return df

# Step 4: Define a function to validate email addresses using regex
def is_valid_email(email):
    # Regex pattern for a valid email
    email_regex = r'^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$'
    if pd.isna(email):  # Handle NaN values
        return False
    return re.match(email_regex, email) is not None

# Step 5: List of columns to remove and add to the garbage file (with underscores and lowercased)
columns_to_garbage = [
    'gender', 'province', 'city', 'address', 'post_code', 'industry',
    'monthly_salary', 'marriage', 'educate', 'brand', 'car',
    'model', 'configuration', 'color'
]

# Step 6: Split the DataFrame into 4 chunks
chunks = np.array_split(df, 4)

# Initialize DataFrames to store all cleaned data and garbage data
all_cleaned_data = pd.DataFrame()
garbage_data = pd.DataFrame()

# Step 7: Clean each chunk, remove specified columns, validate emails, and append invalid rows/columns to garbage data
for chunk in chunks:
    # Drop duplicate rows within the chunk
    chunk.drop_duplicates(inplace=True)

    # Move specified columns to the garbage data, but only those that exist
    available_columns_to_garbage = [col for col in columns_to_garbage if col in chunk.columns]
    chunk_garbage = chunk[available_columns_to_garbage]
    garbage_data = pd.concat([garbage_data, chunk_garbage], axis=0)

    # Drop the garbage columns from the main chunk
    chunk.drop(columns=available_columns_to_garbage, inplace=True)

    # Example: consider rows with NaN values or invalid emails as garbage
    if 'email' in chunk.columns:  # Assuming the email column is named 'email'
        invalid_emails = chunk[~chunk['email'].apply(is_valid_email)]  # Filter invalid emails
        garbage_data = pd.concat([garbage_data, invalid_emails], axis=0)

        # Clean valid rows (including only valid emails)
        valid_rows = chunk[chunk['email'].apply(is_valid_email)]
    else:
        valid_rows = chunk

    # Clean other data in the chunk
    valid_rows_cleaned = clean_data(valid_rows)

    # Append cleaned rows to the all_cleaned_data DataFrame
    all_cleaned_data = pd.concat([all_cleaned_data, valid_rows_cleaned], axis=0)

# Step 8: Save the merged cleaned data and garbage data into separate CSV files
all_cleaned_data.to_csv('merged_cleaned_data.csv', index=False)
garbage_data.to_csv('merged_garbage_data.csv', index=False)

# Download the merged files
files.download('merged_cleaned_data.csv')
files.download('merged_garbage_data.csv')